{
  "cells": [
    {
      "metadata": {
        "id": "jn2WK-EnvdaQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "This notebook is workshop for data prep.\n",
        "\n",
        "\n",
        "\n",
        "#### Agenda\n",
        "*  Loading Libraries\n",
        "*  Loading Data\n",
        "*  Getting Basic Idea About Data\n",
        "*  Summary statistic\n",
        "*  Missing Values and Dealing with Missing Values\n",
        "*  One Hot Encoding (Creating dummies for categorical columns)\n",
        "*  Standardization / Normalization\n",
        "*  Splitting the dataset into train and test data\n",
        "*  Dealing with Imbalanced Data\n",
        "  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "89IQeG9VuUpM"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries\n",
        "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
        "\n",
        "In data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd)."
      ]
    },
    {
      "metadata": {
        "id": "L_lWWBjGvNP1",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd                  # A fundamental package for linear algebra and multidimensional arrays\n",
        "import numpy as np                   # Data analysis and data manipulating tool\n",
        "import random                        # Library to generate random numbers\n",
        "from collections import Counter      # Collection is a Python module that implements specialized container datatypes providing\n",
        "                                     # alternatives to Pythonâ€™s general purpose built-in containers, dict, list, set, and tuple.\n",
        "                                     # Counter is a dict subclass for counting hashable objects\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To ignore warnings in the notebook\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pandas-Profiling\n",
        "!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ],
      "metadata": {
        "id": "3uNtJw_-Q-e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2jIwY1OGvr7A"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "Pandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data.\n",
        "\n",
        "**Disclaimer:** Loading fraud data will take time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload dataset from local\n",
        "from google.colab import files\n",
        "uploades = files.upload()"
      ],
      "metadata": {
        "id": "z4Eyw0WDRB7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "I8GjCJYNRgjz"
      },
      "cell_type": "code",
      "source": [
        "# Loading Data\n",
        "df = pd.read_csv(\"Mall_Customers.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvyUn8YG8wyC"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting Basic Idea About Data"
      ]
    },
    {
      "metadata": {
        "id": "8CmggKjE8ZUk",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwNQSckh83QZ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df.info()      # Returns a concise summary of dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FjL68CPABH7"
      },
      "cell_type": "markdown",
      "source": [
        "There are ....columns with ... observations."
      ]
    },
    {
      "metadata": {
        "id": "GcnguA4n9IsZ",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Taking a look at the target variable\n",
        "df.value_counts(\"\")       # The value_counts() function is used to get a Series containing counts of unique values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sOl5jgYE9wLg",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# we can also use countplot form seaborn to plot the above information graphically.\n",
        "sns.countplot(df[\"\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# summary statistic"
      ],
      "metadata": {
        "id": "OLzhVJZNUmdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(df, title = \"Mall Customers Profiling Report\")\n",
        "profile"
      ],
      "metadata": {
        "id": "9ycoL-o0Uo3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zks_JnKq_6tN"
      },
      "cell_type": "markdown",
      "source": [
        "### Missing values\n",
        "Generally datasets always have some missing values. May be done during data collection, or due to some data validation rule.\n"
      ]
    },
    {
      "metadata": {
        "id": "EHgTEqHR-Kk2",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        " # To get percentage of missing data in each column\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02bqVJEbBxQc"
      },
      "cell_type": "markdown",
      "source": [
        "Out of .... columns, ... have some missing values."
      ]
    },
    {
      "metadata": {
        "id": "qjdy3vG5BmER"
      },
      "cell_type": "markdown",
      "source": [
        "### Dealing with Missing Values\n",
        "*  Filling the missing values with right technique can change our results drastically.\n",
        "*  Also, there is no fixed rule of filling the missing values.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qv5kehT_ALOO",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Dealing with Missing Values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajSX2MhlFbWz",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Let's have a look if there still exist any missing values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LW9XOIz6CGBj"
      },
      "cell_type": "markdown",
      "source": [
        "Notice, now we don't have any column with missing value."
      ]
    },
    {
      "metadata": {
        "id": "qKO21Qg8NSQc"
      },
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoding (Creating dummies for categorical columns)\n",
        "In this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true/false) value to the column. In Python there is a class 'OneHotEncoder' in 'sklearn.preprocessing' to do this task, but here we will use pandas function 'get_dummies()'. This get_dummies() does the same work as done by 'OneHotEncoder' form sklearn.preprocessing.\n"
      ]
    },
    {
      "metadata": {
        "id": "ydD6ZRZrMFFu",
        "trusted": true
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ulgnsjelE3I6",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwytZeEJGyzI"
      },
      "cell_type": "markdown",
      "source": [
        "### Standardization / Normalization\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZEFGJfmobfb0",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaled_features = StandardScaler().fit_transform(X)\n",
        "scaled_features = pd.DataFrame(data=scaled_features)\n",
        "scaled_features.columns= X.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dqiqVCgojsS",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Let's see how the data looks after scaling\n",
        "scaled_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihcN9C3ELFat"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting the dataset into train and test data\n",
        "\n",
        "We will keep 30% of the data for test set."
      ]
    },
    {
      "metadata": {
        "id": "9orHGVW2FwhR",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
        "\n",
        "\n",
        "\n",
        "# test_size = 0.3: 30% of the data will go for test set and 70% of the data will go for train set\n",
        "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify the Imbalanced Data"
      ],
      "metadata": {
        "id": "pvXIRprmUF2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify the Imbalanced Data"
      ],
      "metadata": {
        "id": "JjwbamC8UNoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0myjC1VxpHyx"
      },
      "cell_type": "markdown",
      "source": [
        "# Dealing with Imbalanced Data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1GD5K682rnmo",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Dealing with Imbalanced Data\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}